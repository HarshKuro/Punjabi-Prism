{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ac21a73",
   "metadata": {},
   "source": [
    "# Punjabi Speech Spoof Detection Evaluation\n",
    "## Samsung PRISM Project\n",
    "\n",
    "This notebook implements comprehensive evaluation metrics for spoof detection on the Punjabi speech dataset.\n",
    "\n",
    "**Authors:** Harsh Partap Jain, Gurkirat Singh, Ashmit Singh\n",
    "\n",
    "### Features:\n",
    "- **Baseline**: MFCC + Cosine Similarity to Bonafide Centroid\n",
    "- **Advanced**: LFCC + CQCC Features with Logistic Regression\n",
    "\n",
    "### Evaluation Metrics:\n",
    "- **EER** (Equal Error Rate)\n",
    "- **minDCF** (Minimum Detection Cost Function)\n",
    "- **actDCF** (Actual Detection Cost Function)\n",
    "- **Cllr** (Log-Likelihood Ratio Cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e531e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 1: Import Required Libraries and Configuration\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import scipy.fftpack\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION - Punjabi Dataset Paths\n",
    "# =============================================================================\n",
    "# Update these paths to your dataset location\n",
    "PATH_BONAFIDE = r\"C:\\Users\\Harsh Jain\\Downloads\\prism\\Bonafide\"\n",
    "PATH_SPOOFED_1 = r\"C:\\Users\\Harsh Jain\\Downloads\\prism\\Spoofed-1\"\n",
    "PATH_SPOOFED_2 = r\"C:\\Users\\Harsh Jain\\Downloads\\prism\\Spoofed-2\"\n",
    "\n",
    "# Audio parameters\n",
    "SAMPLE_RATE = 16000\n",
    "MIN_DURATION = 0.2  # Skip files shorter than this (seconds)\n",
    "\n",
    "# Feature extraction parameters\n",
    "N_MFCC = 20\n",
    "N_LFCC = 20\n",
    "N_CQCC = 20\n",
    "N_FFT = 512\n",
    "HOP_LENGTH = 160\n",
    "WIN_LENGTH = 400\n",
    "N_FILTERS = 40\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PUNJABI SPEECH SPOOF DETECTION - Configuration\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Bonafide Path: {PATH_BONAFIDE}\")\n",
    "print(f\"Spoofed-1 Path: {PATH_SPOOFED_1}\")\n",
    "print(f\"Spoofed-2 Path: {PATH_SPOOFED_2}\")\n",
    "print(f\"Sample Rate: {SAMPLE_RATE} Hz\")\n",
    "print(f\"MFCC Features: {N_MFCC}\")\n",
    "print(f\"LFCC Features: {N_LFCC}\")\n",
    "print(f\"CQCC Features: {N_CQCC}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68158569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 2: Utility Functions\n",
    "# =============================================================================\n",
    "\n",
    "def list_audio_files(folder):\n",
    "    \"\"\"Recursively find all audio files in a folder.\"\"\"\n",
    "    extensions = ['wav', 'flac', 'mp3', 'ogg', 'm4a']\n",
    "    files = []\n",
    "    for ext in extensions:\n",
    "        files.extend(glob.glob(os.path.join(folder, '**', f'*.{ext}'), recursive=True))\n",
    "    return sorted(files)\n",
    "\n",
    "\n",
    "def parse_punjabi_filename(filepath):\n",
    "    \"\"\"\n",
    "    Parse Punjabi dataset filename convention.\n",
    "    Format: pa_S{sentence}_{speaker}_{gender}_{device}_{condition}_{distance}_{angle}_{direction}_{noise}_{channel}_{mic}.wav\n",
    "    Example: pa_S01_f1_female_IP14p_na_1m_90_east_57db_0_B.wav\n",
    "    \"\"\"\n",
    "    filename = os.path.basename(filepath)\n",
    "    name_without_ext = os.path.splitext(filename)[0]\n",
    "    parts = name_without_ext.split('_')\n",
    "    \n",
    "    info = {\n",
    "        'filename': filename,\n",
    "        'filepath': filepath,\n",
    "        'language': parts[0] if len(parts) > 0 else 'unknown',\n",
    "        'sentence_id': parts[1] if len(parts) > 1 else 'unknown',\n",
    "        'speaker_id': parts[2] if len(parts) > 2 else 'unknown',\n",
    "        'gender': parts[3] if len(parts) > 3 else 'unknown',\n",
    "        'device': parts[4] if len(parts) > 4 else 'unknown',\n",
    "    }\n",
    "    \n",
    "    # Try to extract distance\n",
    "    for part in parts:\n",
    "        if part.endswith('m') and part[:-1].replace('.', '').isdigit():\n",
    "            info['distance'] = part\n",
    "            break\n",
    "    \n",
    "    return info\n",
    "\n",
    "\n",
    "# Test utility functions\n",
    "print(\"Testing utility functions...\")\n",
    "bona_files = list_audio_files(PATH_BONAFIDE)\n",
    "spoof1_files = list_audio_files(PATH_SPOOFED_1)\n",
    "spoof2_files = list_audio_files(PATH_SPOOFED_2)\n",
    "\n",
    "print(f\"\\nFound {len(bona_files)} bonafide files\")\n",
    "print(f\"Found {len(spoof1_files)} spoofed-1 files\")\n",
    "print(f\"Found {len(spoof2_files)} spoofed-2 files\")\n",
    "\n",
    "if bona_files:\n",
    "    print(f\"\\nSample filename parsing:\")\n",
    "    sample_info = parse_punjabi_filename(bona_files[0])\n",
    "    for k, v in sample_info.items():\n",
    "        print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156f2173",
   "metadata": {},
   "source": [
    "## Feature Extraction Functions\n",
    "\n",
    "Three types of features are extracted:\n",
    "1. **MFCC** - Mel-Frequency Cepstral Coefficients (baseline)\n",
    "2. **LFCC** - Linear Frequency Cepstral Coefficients\n",
    "3. **CQCC** - Constant-Q Cepstral Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc9d864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 3: MFCC Feature Extraction\n",
    "# =============================================================================\n",
    "\n",
    "def extract_mfcc_mean(filepath, sr=SAMPLE_RATE, n_mfcc=N_MFCC):\n",
    "    \"\"\"Extract mean MFCC features from audio file with L2 normalization.\"\"\"\n",
    "    try:\n",
    "        y, _ = librosa.load(filepath, sr=sr, mono=True)\n",
    "        if len(y) < sr * MIN_DURATION:\n",
    "            print(f\"[WARN] File too short: {filepath}\")\n",
    "            return None\n",
    "        \n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "        mfcc_mean = np.mean(mfcc, axis=1)\n",
    "        \n",
    "        # L2 normalize\n",
    "        norm = np.linalg.norm(mfcc_mean)\n",
    "        if norm > 0:\n",
    "            mfcc_mean = mfcc_mean / norm\n",
    "        \n",
    "        return mfcc_mean\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Failed to read {filepath}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Test MFCC extraction\n",
    "if bona_files:\n",
    "    print(\"Testing MFCC extraction...\")\n",
    "    test_mfcc = extract_mfcc_mean(bona_files[0])\n",
    "    if test_mfcc is not None:\n",
    "        print(f\"MFCC shape: {test_mfcc.shape}\")\n",
    "        print(f\"MFCC L2 norm: {np.linalg.norm(test_mfcc):.4f} (should be ~1.0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f94a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 4: LFCC Feature Extraction\n",
    "# =============================================================================\n",
    "\n",
    "def linear_filterbank(n_fft, sr, n_filters=N_FILTERS, fmin=0, fmax=None):\n",
    "    \"\"\"Create linear-spaced filterbank for LFCC.\"\"\"\n",
    "    if fmax is None:\n",
    "        fmax = sr / 2\n",
    "    \n",
    "    freqs = np.linspace(fmin, fmax, n_filters + 2)\n",
    "    bins = np.floor((n_fft + 1) * freqs / sr).astype(int)\n",
    "    \n",
    "    fb = np.zeros((n_filters, n_fft // 2 + 1))\n",
    "    for i in range(1, n_filters + 1):\n",
    "        l, c, r = bins[i-1], bins[i], bins[i+1]\n",
    "        if c > l:\n",
    "            fb[i-1, l:c] = (np.arange(l, c) - l) / (c - l)\n",
    "        if r > c:\n",
    "            fb[i-1, c:r] = (r - np.arange(c, r)) / (r - c)\n",
    "    \n",
    "    return fb\n",
    "\n",
    "\n",
    "def extract_lfcc(y, sr=SAMPLE_RATE, n_fft=N_FFT, hop_length=HOP_LENGTH,\n",
    "                 win_length=WIN_LENGTH, n_filters=N_FILTERS, n_ceps=N_LFCC):\n",
    "    \"\"\"Extract LFCC (Linear Frequency Cepstral Coefficients) features.\"\"\"\n",
    "    S = np.abs(librosa.stft(y, n_fft=n_fft, hop_length=hop_length, win_length=win_length)) ** 2\n",
    "    fb = linear_filterbank(n_fft, sr, n_filters=n_filters)\n",
    "    \n",
    "    feat = np.dot(fb, S[:n_fft // 2 + 1, :])\n",
    "    feat[feat == 0] = 1e-8\n",
    "    log_feat = np.log(feat)\n",
    "    \n",
    "    ceps = scipy.fftpack.dct(log_feat, axis=0, norm='ortho')[:n_ceps, :]\n",
    "    ceps = (ceps - ceps.mean(axis=1, keepdims=True)) / (ceps.std(axis=1, keepdims=True) + 1e-9)\n",
    "    \n",
    "    # Return mean and std as feature vector\n",
    "    feature_vector = np.hstack([ceps.mean(axis=1), ceps.std(axis=1)])\n",
    "    return feature_vector\n",
    "\n",
    "\n",
    "# Test LFCC extraction\n",
    "if bona_files:\n",
    "    print(\"Testing LFCC extraction...\")\n",
    "    y_test, _ = librosa.load(bona_files[0], sr=SAMPLE_RATE, mono=True)\n",
    "    test_lfcc = extract_lfcc(y_test)\n",
    "    print(f\"LFCC feature shape: {test_lfcc.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89288902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 5: CQCC Feature Extraction\n",
    "# =============================================================================\n",
    "\n",
    "def extract_cqcc(y, sr=SAMPLE_RATE, bins_per_octave=24, n_octaves=7, n_ceps=N_CQCC):\n",
    "    \"\"\"Extract CQCC (Constant-Q Cepstral Coefficients) features.\"\"\"\n",
    "    fmin = 20.0\n",
    "    n_bins = n_octaves * bins_per_octave\n",
    "    \n",
    "    C = librosa.cqt(y, sr=sr, hop_length=HOP_LENGTH, fmin=fmin, \n",
    "                    n_bins=n_bins, bins_per_octave=bins_per_octave)\n",
    "    C_mag = np.abs(C)\n",
    "    C_mag[C_mag == 0] = 1e-8\n",
    "    \n",
    "    logC = np.log(C_mag)\n",
    "    ceps = scipy.fftpack.dct(logC, axis=0, norm='ortho')[:n_ceps, :]\n",
    "    ceps = (ceps - ceps.mean(axis=1, keepdims=True)) / (ceps.std(axis=1, keepdims=True) + 1e-9)\n",
    "    \n",
    "    # Return mean and std as feature vector\n",
    "    feature_vector = np.hstack([ceps.mean(axis=1), ceps.std(axis=1)])\n",
    "    return feature_vector\n",
    "\n",
    "\n",
    "def extract_combined_features(filepath, sr=SAMPLE_RATE):\n",
    "    \"\"\"Extract combined LFCC + CQCC features from audio file.\"\"\"\n",
    "    try:\n",
    "        y, _ = librosa.load(filepath, sr=sr, mono=True)\n",
    "        if len(y) < sr * MIN_DURATION:\n",
    "            return None\n",
    "        \n",
    "        lfcc_feat = extract_lfcc(y, sr=sr)\n",
    "        cqcc_feat = extract_cqcc(y, sr=sr)\n",
    "        \n",
    "        return np.hstack([lfcc_feat, cqcc_feat])\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Failed to extract features from {filepath}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Test CQCC and combined extraction\n",
    "if bona_files:\n",
    "    print(\"Testing CQCC extraction...\")\n",
    "    test_cqcc = extract_cqcc(y_test)\n",
    "    print(f\"CQCC feature shape: {test_cqcc.shape}\")\n",
    "    \n",
    "    print(\"\\nTesting combined LFCC+CQCC extraction...\")\n",
    "    test_combined = extract_combined_features(bona_files[0])\n",
    "    if test_combined is not None:\n",
    "        print(f\"Combined feature shape: {test_combined.shape}\")\n",
    "        print(f\"  - LFCC: {N_LFCC * 2} features (mean + std)\")\n",
    "        print(f\"  - CQCC: {N_CQCC * 2} features (mean + std)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799bdded",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "Standard metrics for spoof detection evaluation:\n",
    "- **EER** - Equal Error Rate (where FPR = FNR)\n",
    "- **minDCF** - Minimum Detection Cost Function\n",
    "- **actDCF** - Actual DCF at a given threshold\n",
    "- **Cllr** - Log-Likelihood Ratio Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8564ae34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 6: Evaluation Metrics Functions\n",
    "# =============================================================================\n",
    "\n",
    "def compute_eer(labels, scores, pos_label=1):\n",
    "    \"\"\"\n",
    "    Compute Equal Error Rate (EER).\n",
    "    \n",
    "    Returns:\n",
    "        eer: Equal Error Rate (0-1)\n",
    "        threshold: Threshold at EER\n",
    "        fpr, tpr, thresholds: ROC curve data\n",
    "    \"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(labels, scores, pos_label=pos_label)\n",
    "    fnr = 1 - tpr\n",
    "    \n",
    "    abs_diffs = np.abs(fpr - fnr)\n",
    "    idx = np.nanargmin(abs_diffs)\n",
    "    \n",
    "    eer = (fpr[idx] + fnr[idx]) / 2.0\n",
    "    threshold = thresholds[idx]\n",
    "    \n",
    "    return eer, threshold, fpr, tpr, thresholds\n",
    "\n",
    "\n",
    "def compute_min_dcf(labels, scores, beta=1.9, C_miss=1.0, C_fa=10.0, pos_label=1):\n",
    "    \"\"\"\n",
    "    Compute Minimum Detection Cost Function (minDCF).\n",
    "    \n",
    "    Returns:\n",
    "        min_dcf: Minimum DCF value\n",
    "        threshold: Threshold at minDCF\n",
    "    \"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(labels, scores, pos_label=pos_label)\n",
    "    fnr = 1 - tpr\n",
    "    \n",
    "    dcf_vals = beta * fnr * C_miss + fpr * C_fa\n",
    "    idx = np.argmin(dcf_vals)\n",
    "    \n",
    "    return dcf_vals[idx], thresholds[idx]\n",
    "\n",
    "\n",
    "def compute_act_dcf(labels, scores, threshold, beta=1.9, C_miss=1.0, C_fa=10.0):\n",
    "    \"\"\"\n",
    "    Compute Actual Detection Cost Function (actDCF) at a given threshold.\n",
    "    \n",
    "    Returns:\n",
    "        act_dcf: Actual DCF value\n",
    "        P_miss: Miss probability\n",
    "        P_fa: False alarm probability\n",
    "    \"\"\"\n",
    "    preds = (scores >= threshold).astype(int)\n",
    "    \n",
    "    pos_count = np.sum(labels == 1)\n",
    "    neg_count = np.sum(labels == 0)\n",
    "    \n",
    "    if pos_count == 0 or neg_count == 0:\n",
    "        return float('nan'), None, None\n",
    "    \n",
    "    P_miss = np.sum((labels == 1) & (preds == 0)) / pos_count\n",
    "    P_fa = np.sum((labels == 0) & (preds == 1)) / neg_count\n",
    "    \n",
    "    act_dcf = beta * P_miss * C_miss + P_fa * C_fa\n",
    "    \n",
    "    return act_dcf, P_miss, P_fa\n",
    "\n",
    "\n",
    "def compute_cllr(labels, scores):\n",
    "    \"\"\"\n",
    "    Compute Log-Likelihood Ratio Cost (Cllr) in bits.\n",
    "    Note: Treats scores as LLR-like for baseline.\n",
    "    \"\"\"\n",
    "    s = np.array(scores)\n",
    "    pos = s[labels == 1]\n",
    "    neg = s[labels == 0]\n",
    "    \n",
    "    eps = 1e-300\n",
    "    \n",
    "    if len(pos) == 0 or len(neg) == 0:\n",
    "        return float('nan')\n",
    "    \n",
    "    def log_cost_pos(x):\n",
    "        return np.log(1 + 1 / (np.exp(x) + eps))\n",
    "    \n",
    "    def log_cost_neg(x):\n",
    "        return np.log(1 + np.exp(x))\n",
    "    \n",
    "    cllr = np.mean(log_cost_pos(pos)) + np.mean(log_cost_neg(neg))\n",
    "    cllr_bits = cllr / math.log(2)\n",
    "    \n",
    "    return cllr_bits\n",
    "\n",
    "\n",
    "def compute_cllr_proba(scores, labels):\n",
    "    \"\"\"Compute Cllr from probability scores (0-1 range).\"\"\"\n",
    "    eps = 1e-15\n",
    "    scores = np.clip(scores, eps, 1 - eps)\n",
    "    llr = np.log(scores / (1 - scores))\n",
    "    cllr = np.mean(np.log2(1 + np.exp(-llr * (2 * labels - 1))))\n",
    "    return cllr\n",
    "\n",
    "\n",
    "print(\"Evaluation metric functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabbbda2",
   "metadata": {},
   "source": [
    "## Baseline Evaluation: MFCC + Cosine Similarity\n",
    "\n",
    "This approach:\n",
    "1. Extracts mean MFCC vectors from all audio files\n",
    "2. Computes centroid of bonafide samples\n",
    "3. Scores each sample by cosine similarity to bonafide centroid\n",
    "4. Higher score = more likely genuine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcebd9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 7: Baseline MFCC Evaluation - Spoofed-1 vs Bonafide\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BASELINE MFCC EVALUATION: Spoofed-1 vs Bonafide\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Extract features\n",
    "records = []\n",
    "\n",
    "# Bonafide (label=1)\n",
    "print(\"\\nExtracting MFCC from bonafide files...\")\n",
    "for p in bona_files:\n",
    "    feats = extract_mfcc_mean(p)\n",
    "    if feats is not None:\n",
    "        records.append({'path': p, 'label': 1, 'feat': feats})\n",
    "\n",
    "# Spoofed-1 (label=0)\n",
    "print(\"Extracting MFCC from spoofed-1 files...\")\n",
    "for p in spoof1_files:\n",
    "    feats = extract_mfcc_mean(p)\n",
    "    if feats is not None:\n",
    "        records.append({'path': p, 'label': 0, 'feat': feats})\n",
    "\n",
    "print(f\"\\nTotal samples: {len(records)}\")\n",
    "print(f\"  - Bonafide: {sum(1 for r in records if r['label']==1)}\")\n",
    "print(f\"  - Spoofed: {sum(1 for r in records if r['label']==0)}\")\n",
    "\n",
    "# Compute bonafide centroid\n",
    "feats_pos = np.stack([r['feat'] for r in records if r['label'] == 1])\n",
    "centroid = np.mean(feats_pos, axis=0)\n",
    "norm = np.linalg.norm(centroid)\n",
    "if norm > 0:\n",
    "    centroid = centroid / norm\n",
    "\n",
    "# Score all trials\n",
    "scores_baseline = []\n",
    "labels_baseline = []\n",
    "for r in records:\n",
    "    score = float(cosine_similarity(r['feat'].reshape(1, -1), centroid.reshape(1, -1))[0, 0])\n",
    "    scores_baseline.append(score)\n",
    "    labels_baseline.append(r['label'])\n",
    "\n",
    "scores_baseline = np.array(scores_baseline)\n",
    "labels_baseline = np.array(labels_baseline)\n",
    "\n",
    "# Compute metrics\n",
    "eer, eer_thr, fpr, tpr, _ = compute_eer(labels_baseline, scores_baseline)\n",
    "min_dcf, min_dcf_thr = compute_min_dcf(labels_baseline, scores_baseline)\n",
    "act_dcf, P_miss, P_fa = compute_act_dcf(labels_baseline, scores_baseline, eer_thr)\n",
    "cllr = compute_cllr(labels_baseline, scores_baseline)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BASELINE RESULTS (Spoofed-1 vs Bonafide)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"EER: {eer * 100:.2f}%\")\n",
    "print(f\"EER Threshold: {eer_thr:.4f}\")\n",
    "print(f\"minDCF: {min_dcf:.4f}\")\n",
    "print(f\"actDCF (at EER threshold): {act_dcf:.4f}\")\n",
    "print(f\"Cllr (bits): {cllr:.4f}\")\n",
    "\n",
    "# Store for later comparison\n",
    "baseline_s1_results = {\n",
    "    'eer': eer, 'eer_pct': eer * 100, 'min_dcf': min_dcf, 'cllr': cllr,\n",
    "    'fpr': fpr, 'tpr': tpr\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccad017",
   "metadata": {},
   "source": [
    "## Advanced Evaluation: LFCC + CQCC with Logistic Regression\n",
    "\n",
    "This approach:\n",
    "1. Extracts combined LFCC and CQCC features\n",
    "2. Trains Logistic Regression classifier\n",
    "3. Evaluates with train/test split (80/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db9f508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 8: Advanced LFCC+CQCC Evaluation - Spoofed-1 vs Bonafide\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ADVANCED LFCC+CQCC EVALUATION: Spoofed-1 vs Bonafide\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Extract combined features\n",
    "def load_combined_features(files, label):\n",
    "    X, y = [], []\n",
    "    for f in files:\n",
    "        feat = extract_combined_features(f)\n",
    "        if feat is not None:\n",
    "            X.append(feat)\n",
    "            y.append(label)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "print(\"\\nExtracting LFCC+CQCC from bonafide files...\")\n",
    "X_bona, y_bona = load_combined_features(bona_files, 0)  # Bonafide = 0\n",
    "\n",
    "print(\"Extracting LFCC+CQCC from spoofed-1 files...\")\n",
    "X_spoof, y_spoof = load_combined_features(spoof1_files, 1)  # Spoofed = 1\n",
    "\n",
    "X_adv = np.vstack([X_bona, X_spoof])\n",
    "y_adv = np.hstack([y_bona, y_spoof])\n",
    "\n",
    "print(f\"\\nTotal samples: {len(y_adv)}\")\n",
    "print(f\"Feature dimension: {X_adv.shape[1]}\")\n",
    "\n",
    "# Train/Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_adv, y_adv, test_size=0.2, stratify=y_adv, random_state=42\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\nTrain set: {len(y_train)} samples\")\n",
    "print(f\"Test set: {len(y_test)} samples\")\n",
    "\n",
    "# Train classifier\n",
    "print(\"\\nTraining Logistic Regression...\")\n",
    "clf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get scores\n",
    "scores_adv = clf.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "train_acc = clf.score(X_train_scaled, y_train)\n",
    "test_acc = clf.score(X_test_scaled, y_test)\n",
    "print(f\"Training accuracy: {train_acc * 100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_acc * 100:.2f}%\")\n",
    "\n",
    "# Compute EER\n",
    "fpr_adv, tpr_adv, thresholds_adv = roc_curve(y_test, scores_adv)\n",
    "fnr_adv = 1 - tpr_adv\n",
    "\n",
    "try:\n",
    "    eer_adv = brentq(lambda x: 1. - x - interp1d(fpr_adv, tpr_adv)(x), 0., 1.)\n",
    "    eer_thresh_adv = float(interp1d(fpr_adv, thresholds_adv)(eer_adv))\n",
    "except:\n",
    "    abs_diffs = np.abs(fpr_adv - fnr_adv)\n",
    "    idx = np.nanargmin(abs_diffs)\n",
    "    eer_adv = (fpr_adv[idx] + fnr_adv[idx]) / 2.0\n",
    "    eer_thresh_adv = thresholds_adv[idx]\n",
    "\n",
    "# Compute other metrics\n",
    "def min_dcf_simple(scores, labels, C_miss=1, C_fa=10, pi_spoof=0.05):\n",
    "    thresholds_sorted = np.sort(scores)\n",
    "    min_cost = np.inf\n",
    "    for t in thresholds_sorted:\n",
    "        P_miss = np.mean((scores[labels == 0] < t).astype(float))\n",
    "        P_fa = np.mean((scores[labels == 1] >= t).astype(float))\n",
    "        cost = C_miss * pi_spoof * P_miss + C_fa * (1 - pi_spoof) * P_fa\n",
    "        if cost < min_cost:\n",
    "            min_cost = cost\n",
    "    return min_cost\n",
    "\n",
    "min_dcf_adv = min_dcf_simple(scores_adv, y_test)\n",
    "cllr_adv = compute_cllr_proba(scores_adv, y_test)\n",
    "auc_adv = roc_auc_score(y_test, scores_adv)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ADVANCED RESULTS (Spoofed-1 vs Bonafide)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"EER: {eer_adv * 100:.2f}%\")\n",
    "print(f\"minDCF: {min_dcf_adv:.4f}\")\n",
    "print(f\"Cllr (bits): {cllr_adv:.4f}\")\n",
    "print(f\"AUC: {auc_adv:.4f}\")\n",
    "\n",
    "# Store for comparison\n",
    "advanced_s1_results = {\n",
    "    'eer': eer_adv, 'eer_pct': eer_adv * 100, 'min_dcf': min_dcf_adv, \n",
    "    'cllr': cllr_adv, 'auc': auc_adv, 'fpr': fpr_adv, 'tpr': tpr_adv\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4439b98f",
   "metadata": {},
   "source": [
    "## Visualization: ROC Curves and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4d2a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 9: Visualization - ROC Curves and Confusion Matrix\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# ROC Curve - Baseline\n",
    "ax1 = axes[0]\n",
    "ax1.plot(baseline_s1_results['fpr'], baseline_s1_results['tpr'], 'b-', linewidth=2, label='Baseline MFCC')\n",
    "ax1.plot([0, 1], [0, 1], 'k--', linewidth=1)\n",
    "ax1.scatter([baseline_s1_results['eer']], [1 - baseline_s1_results['eer']], \n",
    "            marker='x', color='red', s=100, linewidths=3,\n",
    "            label=f\"EER = {baseline_s1_results['eer_pct']:.2f}%\")\n",
    "ax1.set_xlabel(\"False Positive Rate\", fontsize=12)\n",
    "ax1.set_ylabel(\"True Positive Rate\", fontsize=12)\n",
    "ax1.set_title(\"ROC Curve - Baseline MFCC\", fontsize=14)\n",
    "ax1.legend(loc='lower right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# ROC Curve - Advanced\n",
    "ax2 = axes[1]\n",
    "ax2.plot(advanced_s1_results['fpr'], advanced_s1_results['tpr'], 'g-', linewidth=2, \n",
    "         label=f\"Advanced LFCC+CQCC (AUC={advanced_s1_results['auc']:.3f})\")\n",
    "ax2.plot([0, 1], [0, 1], 'k--', linewidth=1)\n",
    "ax2.scatter([advanced_s1_results['eer']], [1 - advanced_s1_results['eer']], \n",
    "            marker='x', color='red', s=100, linewidths=3,\n",
    "            label=f\"EER = {advanced_s1_results['eer_pct']:.2f}%\")\n",
    "ax2.set_xlabel(\"False Positive Rate\", fontsize=12)\n",
    "ax2.set_ylabel(\"True Positive Rate\", fontsize=12)\n",
    "ax2.set_title(\"ROC Curve - Advanced LFCC+CQCC\", fontsize=14)\n",
    "ax2.legend(loc='lower right')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Comparison - Both on same plot\n",
    "ax3 = axes[2]\n",
    "ax3.plot(baseline_s1_results['fpr'], baseline_s1_results['tpr'], 'b-', linewidth=2, \n",
    "         label=f\"Baseline (EER={baseline_s1_results['eer_pct']:.1f}%)\")\n",
    "ax3.plot(advanced_s1_results['fpr'], advanced_s1_results['tpr'], 'g-', linewidth=2, \n",
    "         label=f\"Advanced (EER={advanced_s1_results['eer_pct']:.1f}%)\")\n",
    "ax3.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')\n",
    "ax3.set_xlabel(\"False Positive Rate\", fontsize=12)\n",
    "ax3.set_ylabel(\"True Positive Rate\", fontsize=12)\n",
    "ax3.set_title(\"ROC Comparison - Baseline vs Advanced\", fontsize=14)\n",
    "ax3.legend(loc='lower right')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"punjabi_evaluation/roc_comparison_spoofed1.png\", dpi=150, bbox_inches='tight')\n",
    "print(\"Saved ROC comparison to punjabi_evaluation/roc_comparison_spoofed1.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c20c3c",
   "metadata": {},
   "source": [
    "## Full Comparison: Spoofed-1 AND Spoofed-2 Evaluation\n",
    "\n",
    "Run the complete evaluation for both spoof types and compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b675ddbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 10: Full Comparison - Spoofed-1 AND Spoofed-2\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EVALUATING SPOOFED-2 vs BONAFIDE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Baseline for Spoofed-2\n",
    "print(\"\\n--- Baseline MFCC: Spoofed-2 vs Bonafide ---\")\n",
    "records_s2 = []\n",
    "\n",
    "for p in bona_files:\n",
    "    feats = extract_mfcc_mean(p)\n",
    "    if feats is not None:\n",
    "        records_s2.append({'path': p, 'label': 1, 'feat': feats})\n",
    "\n",
    "for p in spoof2_files:\n",
    "    feats = extract_mfcc_mean(p)\n",
    "    if feats is not None:\n",
    "        records_s2.append({'path': p, 'label': 0, 'feat': feats})\n",
    "\n",
    "# Use same centroid from bonafide\n",
    "scores_baseline_s2 = []\n",
    "labels_baseline_s2 = []\n",
    "for r in records_s2:\n",
    "    score = float(cosine_similarity(r['feat'].reshape(1, -1), centroid.reshape(1, -1))[0, 0])\n",
    "    scores_baseline_s2.append(score)\n",
    "    labels_baseline_s2.append(r['label'])\n",
    "\n",
    "scores_baseline_s2 = np.array(scores_baseline_s2)\n",
    "labels_baseline_s2 = np.array(labels_baseline_s2)\n",
    "\n",
    "eer_s2_base, _, fpr_s2_base, tpr_s2_base, _ = compute_eer(labels_baseline_s2, scores_baseline_s2)\n",
    "min_dcf_s2_base, _ = compute_min_dcf(labels_baseline_s2, scores_baseline_s2)\n",
    "cllr_s2_base = compute_cllr(labels_baseline_s2, scores_baseline_s2)\n",
    "\n",
    "print(f\"EER: {eer_s2_base * 100:.2f}%\")\n",
    "print(f\"minDCF: {min_dcf_s2_base:.4f}\")\n",
    "print(f\"Cllr: {cllr_s2_base:.4f}\")\n",
    "\n",
    "baseline_s2_results = {\n",
    "    'eer': eer_s2_base, 'eer_pct': eer_s2_base * 100, \n",
    "    'min_dcf': min_dcf_s2_base, 'cllr': cllr_s2_base,\n",
    "    'fpr': fpr_s2_base, 'tpr': tpr_s2_base\n",
    "}\n",
    "\n",
    "# Advanced for Spoofed-2\n",
    "print(\"\\n--- Advanced LFCC+CQCC: Spoofed-2 vs Bonafide ---\")\n",
    "X_spoof2, y_spoof2 = load_combined_features(spoof2_files, 1)\n",
    "X_adv_s2 = np.vstack([X_bona, X_spoof2])\n",
    "y_adv_s2 = np.hstack([y_bona, y_spoof2])\n",
    "\n",
    "X_train_s2, X_test_s2, y_train_s2, y_test_s2 = train_test_split(\n",
    "    X_adv_s2, y_adv_s2, test_size=0.2, stratify=y_adv_s2, random_state=42\n",
    ")\n",
    "\n",
    "scaler_s2 = StandardScaler()\n",
    "X_train_s2 = scaler_s2.fit_transform(X_train_s2)\n",
    "X_test_s2 = scaler_s2.transform(X_test_s2)\n",
    "\n",
    "clf_s2 = LogisticRegression(max_iter=1000, random_state=42)\n",
    "clf_s2.fit(X_train_s2, y_train_s2)\n",
    "scores_adv_s2 = clf_s2.predict_proba(X_test_s2)[:, 1]\n",
    "\n",
    "fpr_adv_s2, tpr_adv_s2, thresholds_adv_s2 = roc_curve(y_test_s2, scores_adv_s2)\n",
    "try:\n",
    "    eer_adv_s2 = brentq(lambda x: 1. - x - interp1d(fpr_adv_s2, tpr_adv_s2)(x), 0., 1.)\n",
    "except:\n",
    "    fnr_adv_s2 = 1 - tpr_adv_s2\n",
    "    idx = np.nanargmin(np.abs(fpr_adv_s2 - fnr_adv_s2))\n",
    "    eer_adv_s2 = (fpr_adv_s2[idx] + fnr_adv_s2[idx]) / 2.0\n",
    "\n",
    "min_dcf_adv_s2 = min_dcf_simple(scores_adv_s2, y_test_s2)\n",
    "cllr_adv_s2 = compute_cllr_proba(scores_adv_s2, y_test_s2)\n",
    "auc_adv_s2 = roc_auc_score(y_test_s2, scores_adv_s2)\n",
    "\n",
    "print(f\"EER: {eer_adv_s2 * 100:.2f}%\")\n",
    "print(f\"minDCF: {min_dcf_adv_s2:.4f}\")\n",
    "print(f\"Cllr: {cllr_adv_s2:.4f}\")\n",
    "print(f\"AUC: {auc_adv_s2:.4f}\")\n",
    "\n",
    "advanced_s2_results = {\n",
    "    'eer': eer_adv_s2, 'eer_pct': eer_adv_s2 * 100,\n",
    "    'min_dcf': min_dcf_adv_s2, 'cllr': cllr_adv_s2, 'auc': auc_adv_s2,\n",
    "    'fpr': fpr_adv_s2, 'tpr': tpr_adv_s2\n",
    "}\n",
    "\n",
    "# =============================================\n",
    "# SUMMARY COMPARISON TABLE\n",
    "# =============================================\n",
    "print(\"\\n\\n\" + \"=\" * 70)\n",
    "print(\"SUMMARY COMPARISON - ALL EVALUATIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "summary_data = [\n",
    "    {'Evaluation': 'Spoofed-1 (Baseline MFCC)', 'EER (%)': baseline_s1_results['eer_pct'], \n",
    "     'minDCF': baseline_s1_results['min_dcf'], 'Cllr (bits)': baseline_s1_results['cllr']},\n",
    "    {'Evaluation': 'Spoofed-1 (Advanced LFCC+CQCC)', 'EER (%)': advanced_s1_results['eer_pct'], \n",
    "     'minDCF': advanced_s1_results['min_dcf'], 'Cllr (bits)': advanced_s1_results['cllr']},\n",
    "    {'Evaluation': 'Spoofed-2 (Baseline MFCC)', 'EER (%)': baseline_s2_results['eer_pct'], \n",
    "     'minDCF': baseline_s2_results['min_dcf'], 'Cllr (bits)': baseline_s2_results['cllr']},\n",
    "    {'Evaluation': 'Spoofed-2 (Advanced LFCC+CQCC)', 'EER (%)': advanced_s2_results['eer_pct'], \n",
    "     'minDCF': advanced_s2_results['min_dcf'], 'Cllr (bits)': advanced_s2_results['cllr']},\n",
    "]\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\n\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Save summary\n",
    "summary_df.to_csv(\"punjabi_evaluation/summary_comparison.csv\", index=False)\n",
    "print(\"\\n\\nSaved summary to punjabi_evaluation/summary_comparison.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15873433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 11: Final Visualization - Complete Comparison\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# Spoofed-1 Baseline\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(baseline_s1_results['fpr'], baseline_s1_results['tpr'], 'b-', linewidth=2)\n",
    "ax1.plot([0, 1], [0, 1], 'k--', linewidth=1)\n",
    "ax1.scatter([baseline_s1_results['eer']], [1 - baseline_s1_results['eer']], \n",
    "            marker='x', color='red', s=100, linewidths=3)\n",
    "ax1.set_xlabel(\"FPR\")\n",
    "ax1.set_ylabel(\"TPR\")\n",
    "ax1.set_title(f\"Spoofed-1 Baseline (EER={baseline_s1_results['eer_pct']:.1f}%)\")\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Spoofed-1 Advanced\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(advanced_s1_results['fpr'], advanced_s1_results['tpr'], 'g-', linewidth=2)\n",
    "ax2.plot([0, 1], [0, 1], 'k--', linewidth=1)\n",
    "ax2.scatter([advanced_s1_results['eer']], [1 - advanced_s1_results['eer']], \n",
    "            marker='x', color='red', s=100, linewidths=3)\n",
    "ax2.set_xlabel(\"FPR\")\n",
    "ax2.set_ylabel(\"TPR\")\n",
    "ax2.set_title(f\"Spoofed-1 Advanced (EER={advanced_s1_results['eer_pct']:.1f}%)\")\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Spoofed-2 Baseline\n",
    "ax3 = axes[1, 0]\n",
    "ax3.plot(baseline_s2_results['fpr'], baseline_s2_results['tpr'], 'b-', linewidth=2)\n",
    "ax3.plot([0, 1], [0, 1], 'k--', linewidth=1)\n",
    "ax3.scatter([baseline_s2_results['eer']], [1 - baseline_s2_results['eer']], \n",
    "            marker='x', color='red', s=100, linewidths=3)\n",
    "ax3.set_xlabel(\"FPR\")\n",
    "ax3.set_ylabel(\"TPR\")\n",
    "ax3.set_title(f\"Spoofed-2 Baseline (EER={baseline_s2_results['eer_pct']:.1f}%)\")\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Spoofed-2 Advanced\n",
    "ax4 = axes[1, 1]\n",
    "ax4.plot(advanced_s2_results['fpr'], advanced_s2_results['tpr'], 'g-', linewidth=2)\n",
    "ax4.plot([0, 1], [0, 1], 'k--', linewidth=1)\n",
    "ax4.scatter([advanced_s2_results['eer']], [1 - advanced_s2_results['eer']], \n",
    "            marker='x', color='red', s=100, linewidths=3)\n",
    "ax4.set_xlabel(\"FPR\")\n",
    "ax4.set_ylabel(\"TPR\")\n",
    "ax4.set_title(f\"Spoofed-2 Advanced (EER={advanced_s2_results['eer_pct']:.1f}%)\")\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"Punjabi Spoof Detection - ROC Curves Comparison\\nSamsung PRISM Project\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"punjabi_evaluation/full_roc_comparison.png\", dpi=150, bbox_inches='tight')\n",
    "print(\"Saved full ROC comparison to punjabi_evaluation/full_roc_comparison.png\")\n",
    "plt.show()\n",
    "\n",
    "# Bar chart comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "methods = ['S1-Baseline', 'S1-Advanced', 'S2-Baseline', 'S2-Advanced']\n",
    "eer_values = [baseline_s1_results['eer_pct'], advanced_s1_results['eer_pct'],\n",
    "              baseline_s2_results['eer_pct'], advanced_s2_results['eer_pct']]\n",
    "colors = ['#3498db', '#2ecc71', '#3498db', '#2ecc71']\n",
    "\n",
    "bars = ax.bar(methods, eer_values, color=colors, edgecolor='black', linewidth=1.5)\n",
    "ax.set_ylabel(\"EER (%)\", fontsize=12)\n",
    "ax.set_title(\"Equal Error Rate Comparison\\nPunjabi Spoof Detection\", fontsize=14)\n",
    "ax.set_ylim(0, max(eer_values) * 1.2)\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars, eer_values):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "            f'{val:.1f}%', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"punjabi_evaluation/eer_comparison_bar.png\", dpi=150, bbox_inches='tight')\n",
    "print(\"Saved EER bar chart to punjabi_evaluation/eer_comparison_bar.png\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EVALUATION COMPLETE!\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nOutput files saved to: punjabi_evaluation/\")\n",
    "print(\"  - summary_comparison.csv\")\n",
    "print(\"  - roc_comparison_spoofed1.png\")\n",
    "print(\"  - full_roc_comparison.png\")\n",
    "print(\"  - eer_comparison_bar.png\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
